{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from datasets.reader import mnist as mnist_reader\n",
    "slim = tf.contrib.slim\n",
    "layers = tf.contrib.layers\n",
    "from tensorflow.python.ops import variable_scope\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adult_height = np.random.normal(175, 5, [100, 1])\n",
    "adult_weight = np.random.normal(70, 5, [100, 1])\n",
    "\n",
    "adult_dataset = np.concatenate( (adult_weight, adult_height) , axis = 1)\n",
    "\n",
    "print(adult_dataset.shape)\n",
    "print(adult_dataset[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "child_height = np.random.normal(120, 5, [100, 1])\n",
    "child_weight = np.random.normal(30, 5, [100, 1])\n",
    "\n",
    "child_dataset = np.concatenate( (child_weight, child_height) , axis = 1)\n",
    "\n",
    "print(child_dataset.shape)\n",
    "print(child_dataset[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.scatter(adult_dataset[:,0], adult_dataset[:,1])\n",
    "ax1.scatter(child_dataset[:,0], child_dataset[:,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adult_label = np.ones( shape=[100, 1] )\n",
    "child_label = np.zeros( shape=[100, 1] )\n",
    "label = np.concatenate( (adult_label, child_label) )\n",
    "print('label의 shape' , label.shape)\n",
    "print(label[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset + Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_dataset = np.concatenate((adult_dataset , child_dataset))\n",
    "total_dataset = np.concatenate( (total_dataset, label), axis = 1  )\n",
    "\n",
    "np.random.shuffle(total_dataset) # Shuffle dataset\n",
    "print(total_dataset[:10])\n",
    "print(total_dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# simple scaling of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_mean= total_dataset[:, 0].mean()\n",
    "height_mean= total_dataset[:, 1].mean()\n",
    "total_dataset[:, 0] /= weight_mean\n",
    "total_dataset[:, 1] /= height_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_dataset.shape\n",
    "\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.scatter(total_dataset[:,0], total_dataset[:,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regressor(data):\n",
    "    logits = layers.fully_connected(data, 1, activation_fn=None) # logits = W1 * x1 + W2 * x2 + b\n",
    "    prob = tf.nn.sigmoid(logits)\n",
    "    return logits, prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Graph().as_default():\n",
    "    real_data = tf.constant(total_dataset[:, :2], tf.float32)\n",
    "    label = tf.constant(total_dataset[:, 2], tf.float32)\n",
    "    label = tf.reshape(label, [-1, 1])\n",
    "    with variable_scope.variable_scope('logistic_regressor') as dis_scope:\n",
    "        logits, prob = logistic_regressor(real_data)    \n",
    "    \n",
    "    loss1 = -tf.multiply(label,tf.log(prob + 0.0006)) # -P(x)*log(Q(x))\n",
    "    loss2 = -tf.multiply( 1.-label, tf.log(1. - prob + 0.0006) ) # -(1-P(x))log(1-Q(x))\n",
    "    loss = loss1 + loss2 # -P(x)*log(Q(x)) -(1-P(x))log(1-Q(x))\n",
    "    D_loss = tf.reduce_mean(loss)\n",
    "    \n",
    "    initializer = tf.global_variables_initializer()\n",
    "    \n",
    "    dis_var = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=dis_scope.name)\n",
    "    D_solver = tf.train.AdamOptimizer(learning_rate=0.01).minimize(D_loss, var_list=dis_var)\n",
    "    initializer = tf.global_variables_initializer()\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(initializer)\n",
    "\n",
    "        for i in range(10000):\n",
    "            loss, _ = sess.run([D_loss, D_solver])\n",
    "            if i % 100 == 0:\n",
    "                print('iteration : ', i, 'loss : ', loss)\n",
    "            if loss < 0.005:\n",
    "                break\n",
    "    \n",
    "        with variable_scope.variable_scope('logistic_regressor', reuse=True):\n",
    "            weight1 = tf.get_variable('fully_connected/weights')\n",
    "            bias2 = tf.get_variable('fully_connected/biases')\n",
    "            w = sess.run(weight1)\n",
    "            b = sess.run(bias2)\n",
    "            print('dis_weight', w)\n",
    "            print('dis_bias', b)\n",
    "            \n",
    "            # w[0]*x1 + w[1]*x2 + b = 0\n",
    "            # p1 = (0, -b/w[1])\n",
    "            # p2 = (-b/w[0], 0)\n",
    "            \n",
    "            p1 = (0, -b/w[1])\n",
    "            p2 = (-b/w[0], 0)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(p1, p2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.scatter(total_dataset[:,0], total_dataset[:,1])\n",
    "\n",
    "coefficients = np.polyfit([p1[0], p2[0]], [p1[1], p2[1]], 1) \n",
    "polynomial = np.poly1d(coefficients)\n",
    "x_axis = np.linspace(0, 2)\n",
    "y_axis = polynomial(x_axis)\n",
    "\n",
    "ax1.plot(x_axis, y_axis)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-class linear classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_height = np.random.normal(175, 5, [100, 1])\n",
    "new_weight = np.random.normal(30, 5, [100, 1])\n",
    "\n",
    "new_class_dataset = np.concatenate( (new_weight, new_height) , axis = 1)\n",
    "\n",
    "print(new_class_dataset.shape)\n",
    "print(new_class_dataset[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.scatter(adult_dataset[:,0], adult_dataset[:,1])\n",
    "ax1.scatter(child_dataset[:,0], child_dataset[:,1])\n",
    "ax1.scatter(new_class_dataset[:, 0], new_class_dataset[:, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data combining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_dataset = np.concatenate((adult_dataset , child_dataset, new_class_dataset))\n",
    "label = np.concatenate( (adult_label, child_label, np.ones( shape=[100, 1] )*2 ) )\n",
    "                                   \n",
    "total_dataset = np.concatenate( (total_dataset, label), axis = 1  )\n",
    "\n",
    "np.random.shuffle(total_dataset) # Shuffle dataset\n",
    "print(total_dataset[:10])\n",
    "print(total_dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_mean = total_dataset[:, 0].mean()\n",
    "height_mean = total_dataset[:, 1].mean()\n",
    "total_dataset[:, 0] -=weight_mean\n",
    "total_dataset[:, 0] /=weight_mean\n",
    "total_dataset[:, 1]-=(height_mean/3)\n",
    "total_dataset[:, 1]/=(height_mean/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_dataset.shape\n",
    "\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.scatter(total_dataset[:,0], total_dataset[:,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier(data, num_class):\n",
    "    logits = layers.fully_connected(data, 3, activation_fn=None) # logits = W1 * x1 + W2 * x2 + b\n",
    "    #prob = tf.nn.sigmoid(logits)\n",
    "    return logits #scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Graph().as_default():\n",
    "    real_data = tf.constant(total_dataset[:, :2], tf.float32)\n",
    "    label = tf.constant(total_dataset[:, 2], tf.float32)\n",
    "    label = tf.reshape(label, [-1, 1])\n",
    "    with variable_scope.variable_scope('classifier') as dis_scope:\n",
    "        logits = classifier(real_data, num_class = 3)    \n",
    "    \n",
    "    onehot_encoded = tf.one_hot( tf.cast(label, tf.uint8) , depth = 3 )\n",
    "    \n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(labels = onehot_encoded, logits = logits)\n",
    "    D_loss = tf.reduce_mean(loss)\n",
    "\n",
    "    dis_var = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=dis_scope.name)\n",
    "    #학습할 variable을 scope로 지정해서 아래 optimizer에서 scope 내의 variable만 update할 수 있다.\n",
    "    D_solver = tf.train.AdamOptimizer(learning_rate=0.001).minimize(D_loss, var_list=dis_var) \n",
    "    \n",
    "    initializer = tf.global_variables_initializer()\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(initializer)\n",
    "\n",
    "        for i in range(50000):\n",
    "            loss, _ = sess.run([D_loss, D_solver])\n",
    "            if i % 1000 == 0:\n",
    "                print('iteration : ', i, 'loss : ', loss)\n",
    "            if loss < 0.0005:\n",
    "                break\n",
    "        print(\"\\n\")\n",
    "        print(\"Label : \\n\")\n",
    "        print(sess.run(label[:10]))\n",
    "        print(\"probability : \\n\")\n",
    "        print(sess.run(tf.nn.softmax(logits)[:10]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
